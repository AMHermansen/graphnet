"""Classification-specific `Model` class(es)."""
from pathlib import Path
from typing import Any, Union, List, Optional, Dict, Tuple, Type, Callable

import pandas as pd
import torch
from torch import Tensor, nn
from torch_geometric.data import Data
from torch_geometric.utils import to_dense_batch
from torchmetrics import Metric, Accuracy, AUROC

from graphnet.models.task import Task, IdentityTask
from graphnet.constants import STYLESHEET_PATH
from graphnet.plot.classification import default_hist_plot, default_roc_curve
from graphnet.plot.utils import LogitHistData, ROCCurveData


class MulticlassClassificationTask(IdentityTask):
    """Task used for multi-classification."""

    def __init__(
        self,
        nb_outputs: int,
        target_labels: Union[List[str], Any],
        prediction_labels: Optional[List[str]] = None,
        logit_hist_data: Optional[LogitHistData] = None,
        roc_curve_data: Optional[ROCCurveData] = None,
        *args: Any,
        **kwargs: Any,
    ):
        """Construct Multi Classification Task module.

        Args:
            nb_outputs: Number of outputs.
            target_labels: Target labels.
            prediction_labels: Labels used for prediction, defaults to labels generated by IdentityTask.
            logit_hist_data: Data for plotting logit histograms.
            roc_curve_data: Data for plotting ROC curves.
            *args:
            **kwargs:
        """
        super().__init__(nb_outputs, target_labels, *args, **kwargs)
        self._logit_hist_data = logit_hist_data or LogitHistData()
        self._roc_curve_data = roc_curve_data or ROCCurveData(  # type: ignore
            legend_labels=[f"Class {i}" for i in range(nb_outputs)]  # type: ignore
        )  # type: ignore
        if prediction_labels is not None:
            self._default_prediction_labels = prediction_labels
            self._prediction_labels = prediction_labels

        if nb_outputs != len(self._default_prediction_labels):
            self.warning(
                "Specified number of outputs and number of prediction labels don't match. Result saving might get corrupted."
            )
        if nb_outputs != len(self._roc_curve_data.legend_labels):
            self.warning(
                "Specified number of outputs and number of ROC curve labels don't match. ROC curve plotting might get corrupted."
            )

    """General task for classifying any number of classes.

    Requires the same number of input features as the number of classes being
    predicted. Returns the untransformed latent features, which are interpreted
    as the logits for each class being classified.
    """

    def plot(
        self,
        cache: pd.DataFrame,
        output_dir: str,
        epoch: Optional[int] = None,
        style: Optional[Union[List[str], dict, Path]] = None,
        output_prefix: Optional[str] = "",
    ) -> bool:
        """Create histograms of the logit values and ROC curves.

        Args:
            cache: Cache of results from the model.
            output_dir: Directory to save the plot to.
            epoch: The current epoch, used for naming the plot.
            style: Path to a preferred stylesheet for the plot.
            output_prefix: Prefix to add to the plot name.
        Returns: True if all plots were successfully created, False if ROCCurve failed.
        """
        style = style or [STYLESHEET_PATH]
        epoch_str = f"_epoch_{epoch}" if epoch is not None else ""

        maybe_target_ids = self._loss_function.convert_to_ids(  # type: ignore
            cache[self._target_labels[0]]  # Only expect one target label.
        )
        if maybe_target_ids is None:
            return False

        common_hist_kwargs = dict(
            cache=cache,
            prediction_labels=self._prediction_labels,
            target_ids=maybe_target_ids,
            output_dir=output_dir,
            style=style,
            logit_hist_data=self._logit_hist_data,
            file_suffix=epoch_str,
            file_prefix=output_prefix,
        )
        default_hist_plot(
            **common_hist_kwargs,
            y_log=True,
        )
        default_hist_plot(
            **common_hist_kwargs,
            y_log=False,
        )
        common_roc_kwargs = dict(
            predictions=cache[self._default_prediction_labels],
            target_ids=maybe_target_ids,
            output_dir=output_dir,
            style=style,
            roc_curve_data=self._roc_curve_data,
            file_suffix=epoch_str,
            file_prefix=output_prefix,
        )
        default_roc_curve(
            **common_roc_kwargs,
            y_log=False,
        )
        default_roc_curve(
            **common_roc_kwargs,
            y_log=True,
        )
        self.info(f"Plots saved to {output_dir}")
        return True


class BinaryClassificationTask(Task):
    """Performs binary classification."""

    # Requires one feature, logit for being signal class.
    nb_inputs = 1
    default_target_labels = ["target"]
    default_prediction_labels = ["target_pred"]

    def _forward(self, x: Tensor) -> Tensor:
        # transform probability of being muon
        return torch.sigmoid(x)


class BinaryClassificationTaskLogits(IdentityTask):
    """Performs binary classification form logits."""
    def __init__(self,
                 target_labels: Union[List[str], Any],
                 prediction_labels: Optional[List[str]] = None,
                 *args,
                 **kwargs
                 ):
        super().__init__(nb_outputs=1, target_labels=target_labels, *args, **kwargs)

        if prediction_labels is not None:
            self._default_prediction_labels = prediction_labels
            self._prediction_labels = prediction_labels

    def _forward(self, x: Tensor) -> Tensor:
        return x


class MAEClassificationTask(Task):
    default_target_labels = ["sensor_id"]
    _forward = nn.Identity()

    def __init__(
            self,
            hidden_size: int,
            number_of_sensors: int = 5484,
    ):
        from graphnet.training.loss_functions import CrossEntropyLoss
        self._number_of_sensors = number_of_sensors
        super().__init__(
            hidden_size=hidden_size,
            loss_function=CrossEntropyLoss(self._number_of_sensors)
        )

    @property
    def default_prediction_labels(self) -> List[str]:
        return [f"sensor_{i}_logit" for i in range(self._number_of_sensors)]

    @property
    def nb_inputs(self) -> int:
        return self._number_of_sensors

    @property
    def default_metrics(self) -> Dict[str, Tuple[Union[Type[Metric], Callable[[], Metric]], bool]]:
        return {
            "acc1": (lambda: Accuracy(task="multiclass", num_classes=self._number_of_sensors, top_k=1), True),
            "acc10": (lambda: Accuracy(task="multiclass", num_classes=self._number_of_sensors, top_k=10), True),
            "acc100": (lambda: Accuracy(task="multiclass", num_classes=self._number_of_sensors, top_k=100), True),
            "acc1000": (lambda: Accuracy(task="multiclass", num_classes=self._number_of_sensors, top_k=1000), True),
            # "auc": (lambda: AUROC(task="multiclass", num_classes=self._number_of_sensors), True)
        }

    def compute_loss(self, pred: Tensor, data: Data) -> Tensor:
        sensor_id = data.sensor_id
        batch = data.old_batch
        sensor_id, _ = to_dense_batch(sensor_id, batch, fill_value=-1)
        sensor_id = sensor_id.to(torch.int64)
        ae_mask = data.ae_mask

        loss = 0.
        for pred_event, sensor_id_event, ae_mask_event in zip(
                pred, sensor_id, ae_mask
        ):
            sensor_id_event = sensor_id_event[ae_mask_event]
            pred_event = pred_event.repeat(sensor_id_event.shape[0], 1)
            loss += self._loss_function(pred_event, sensor_id_event)
        return loss / pred.shape[0]

    def compute_metrics(self, pred: Union[Tensor, Data], data: Data, train: bool = True):
        sensor_id = data.sensor_id
        batch = data.old_batch
        sensor_id, _ = to_dense_batch(sensor_id, batch, fill_value=-1)
        sensor_id = sensor_id.to(torch.int64)
        ae_mask = data.ae_mask

        for pred_event, sensor_id_event, ae_mask_event in zip(
                pred, sensor_id, ae_mask
        ):
            sensor_id_event = sensor_id_event[ae_mask_event]
            pred_event = pred_event.repeat(sensor_id_event.shape[0], 1)
            metrics = self.train_metrics if train else self.val_metrics
            for metric in metrics.values():
                metric(pred_event, sensor_id_event)
